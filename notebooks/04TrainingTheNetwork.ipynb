{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "training_intro",
   "metadata": {},
   "source": [
    "# Training the Regulatory Network\n",
    "\n",
    "Train a neural network to act as the regulatory function $f(\\bar{s})$ using **evolutionary optimization**. Goal: Maximize utility $U = S_{pat} - S_{rep}$ to discover patterns with high information content and low variability.\n",
    "\n",
    "**Expected outcome**: Network converges to tanh-like lateral inhibition → alternating on-off patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Import our modules\n",
    "from neural_network import RegulatoryNetwork, init_params, get_regulatory_function\n",
    "from dynamics import run_multiple_replicates, apply_threshold\n",
    "from utility_function import compute_soft_utility, compute_hard_utility\n",
    "\n",
    "# Plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Create figures directory\n",
    "Path('../figures').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params_header",
   "metadata": {},
   "source": [
    "## Simulation & Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System parameters\n",
    "N_CELLS = 7                # Number of cells in 1D system\n",
    "N_REPLICATES = 50          # Replicates per fitness evaluation (more = more reliable)\n",
    "DT = 0.01                  # Time step\n",
    "T = 20.0                   # Simulation time\n",
    "N_STEPS = int(T / DT)      # Number of integration steps\n",
    "NOISE_STRENGTH = 0.1       # Stochastic noise level\n",
    "\n",
    "# Neural network architecture\n",
    "HIDDEN_DIMS = (8, 8)       # Two hidden layers with 8 neurons each\n",
    "\n",
    "# Evolution parameters\n",
    "POPULATION_SIZE = 20       # Number of individuals per generation\n",
    "N_GENERATIONS = 100        # Total training iterations\n",
    "MUTATION_STD = 0.1         # Standard deviation for parameter mutations\n",
    "ELITE_FRACTION = 0.2       # Keep top 20% as parents\n",
    "\n",
    "# Utility function parameters\n",
    "SOFT_BANDWIDTH = 0.1       # KDE bandwidth for soft utility\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "key = random.PRNGKey(SEED)\n",
    "\n",
    "print(f\"System: {N_CELLS} cells, {N_REPLICATES} replicates\")\n",
    "print(f\"Dynamics: T={T}s, dt={DT}, noise={NOISE_STRENGTH}\")\n",
    "print(f\"Network: hidden_dims={HIDDEN_DIMS}\")\n",
    "print(f\"Evolution: pop={POPULATION_SIZE}, gen={N_GENERATIONS}, σ_mut={MUTATION_STD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evolution_header",
   "metadata": {},
   "source": [
    "## Evolution Strategy Implementation\n",
    "\n",
    "Simple **(μ+λ)-ES** with Gaussian mutations:\n",
    "1. Evaluate fitness (utility) for each individual\n",
    "2. Select elite parents (top performers)\n",
    "3. Generate offspring via mutation\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_params(params: Dict) -> jnp.ndarray:\n",
    "    \"\"\"Flatten nested parameter dict to 1D array for evolution.\"\"\"\n",
    "    flat = []\n",
    "    for layer_params in jax.tree_util.tree_leaves(params):\n",
    "        flat.append(layer_params.flatten())\n",
    "    return jnp.concatenate(flat)\n",
    "\n",
    "def unflatten_params(flat: jnp.ndarray, template: Dict) -> Dict:\n",
    "    \"\"\"Reconstruct parameter dict from flat array using template structure.\"\"\"\n",
    "    # Get shapes from template\n",
    "    shapes = [x.shape for x in jax.tree_util.tree_leaves(template)]\n",
    "    sizes = [np.prod(s) for s in shapes]\n",
    "    \n",
    "    # Split flat array\n",
    "    idx = 0\n",
    "    arrays = []\n",
    "    for size, shape in zip(sizes, shapes):\n",
    "        arrays.append(flat[idx:idx+size].reshape(shape))\n",
    "        idx += size\n",
    "    \n",
    "    # Reconstruct tree structure\n",
    "    return jax.tree_util.tree_unflatten(jax.tree_util.tree_structure(template), arrays)\n",
    "\n",
    "# Test parameter flattening\n",
    "model = RegulatoryNetwork(hidden_dims=HIDDEN_DIMS)\n",
    "key, subkey = random.split(key)\n",
    "test_params = init_params(model, subkey, (1,))\n",
    "flat = flatten_params(test_params)\n",
    "reconstructed = unflatten_params(flat, test_params)\n",
    "\n",
    "print(f\"✓ Parameter flattening working\")\n",
    "print(f\"  Total parameters: {len(flat)}\")\n",
    "print(f\"  Structure: {jax.tree_util.tree_map(lambda x: x.shape, test_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitness_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_fitness(params: Dict, model: RegulatoryNetwork, eval_key: jax.random.PRNGKey) -> float:\n",
    "    \"\"\"Evaluate utility (fitness) for given network parameters.\n",
    "    \n",
    "    Higher utility = better fitness.\n",
    "    Uses soft utility for differentiable patterns (though ES doesn't need gradients).\n",
    "    \"\"\"\n",
    "    # Get regulatory function from params\n",
    "    f = get_regulatory_function(model, params)\n",
    "    \n",
    "    # Run simulations\n",
    "    final_states = run_multiple_replicates(\n",
    "        f=f,\n",
    "        n_cells=N_CELLS,\n",
    "        n_replicates=N_REPLICATES,\n",
    "        n_steps=N_STEPS,\n",
    "        dt=DT,\n",
    "        noise_strength=NOISE_STRENGTH,\n",
    "        key=eval_key\n",
    "    )\n",
    "    \n",
    "    # Apply threshold to get patterns (using STE for technical consistency)\n",
    "    patterns = apply_threshold(final_states)\n",
    "    \n",
    "    # Compute soft utility (allows gradients to flow, even though ES doesn't use them)\n",
    "    utility, s_pat, s_rep = compute_soft_utility(patterns, bandwidth=SOFT_BANDWIDTH)\n",
    "    \n",
    "    return utility\n",
    "\n",
    "# Test fitness computation\n",
    "key, subkey = random.split(key)\n",
    "test_fitness = compute_fitness(test_params, model, subkey)\n",
    "print(f\"✓ Fitness function working\")\n",
    "print(f\"  Test fitness (random params): {test_fitness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evolution_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(population: jnp.ndarray, model: RegulatoryNetwork, \n",
    "                       template: Dict, eval_key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "    \"\"\"Evaluate fitness for entire population.\"\"\"\n",
    "    fitnesses = []\n",
    "    keys = random.split(eval_key, len(population))\n",
    "    \n",
    "    for individual, key_i in zip(population, keys):\n",
    "        params = unflatten_params(individual, template)\n",
    "        fitness = compute_fitness(params, model, key_i)\n",
    "        fitnesses.append(fitness)\n",
    "    \n",
    "    return jnp.array(fitnesses)\n",
    "\n",
    "def select_parents(population: jnp.ndarray, fitnesses: jnp.ndarray, \n",
    "                  n_parents: int) -> jnp.ndarray:\n",
    "    \"\"\"Select top performers as parents.\"\"\"\n",
    "    parent_indices = jnp.argsort(fitnesses)[-n_parents:]  # Top n_parents\n",
    "    return population[parent_indices]\n",
    "\n",
    "def mutate_population(parents: jnp.ndarray, population_size: int, \n",
    "                     mutation_std: float, mut_key: jax.random.PRNGKey) -> jnp.ndarray:\n",
    "    \"\"\"Generate offspring via Gaussian mutation of parents.\"\"\"\n",
    "    n_parents = len(parents)\n",
    "    n_children = population_size - n_parents\n",
    "    \n",
    "    # Random parent selection for each child\n",
    "    parent_indices = random.randint(mut_key, (n_children,), 0, n_parents)\n",
    "    \n",
    "    # Generate mutations\n",
    "    noise_key = random.split(mut_key)[1]\n",
    "    noise = random.normal(noise_key, (n_children, parents.shape[1])) * mutation_std\n",
    "    \n",
    "    children = parents[parent_indices] + noise\n",
    "    \n",
    "    # Combine parents and children\n",
    "    return jnp.concatenate([parents, children], axis=0)\n",
    "\n",
    "print(\"✓ Evolution functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Evolve population over multiple generations, tracking best fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize population (random parameters)\n",
    "print(\"Initializing population...\")\n",
    "param_template = init_params(model, random.PRNGKey(0), (1,))\n",
    "n_params = len(flatten_params(param_template))\n",
    "n_parents = max(2, int(POPULATION_SIZE * ELITE_FRACTION))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "population_keys = random.split(subkey, POPULATION_SIZE)\n",
    "population = jnp.array([flatten_params(init_params(model, k, (1,))) \n",
    "                       for k in population_keys])\n",
    "\n",
    "print(f\"Population: {POPULATION_SIZE} individuals, {n_params} params each\")\n",
    "print(f\"Parents per generation: {n_parents}\\n\")\n",
    "\n",
    "# Training history\n",
    "best_fitness_history = []\n",
    "mean_fitness_history = []\n",
    "best_params = None\n",
    "best_fitness = -jnp.inf\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"Gen | Best Fit | Mean Fit | Std Fit\")\n",
    "print(\"----+----------+----------+---------\")\n",
    "\n",
    "for generation in range(N_GENERATIONS):\n",
    "    # Evaluate fitness\n",
    "    key, eval_key, mut_key = random.split(key, 3)\n",
    "    fitnesses = evaluate_population(population, model, param_template, eval_key)\n",
    "    \n",
    "    # Track best\n",
    "    gen_best_fitness = float(jnp.max(fitnesses))\n",
    "    gen_mean_fitness = float(jnp.mean(fitnesses))\n",
    "    gen_std_fitness = float(jnp.std(fitnesses))\n",
    "    \n",
    "    best_fitness_history.append(gen_best_fitness)\n",
    "    mean_fitness_history.append(gen_mean_fitness)\n",
    "    \n",
    "    if gen_best_fitness > best_fitness:\n",
    "        best_fitness = gen_best_fitness\n",
    "        best_idx = jnp.argmax(fitnesses)\n",
    "        best_params = population[best_idx]\n",
    "    \n",
    "    # Print progress\n",
    "    if generation % 10 == 0 or generation == N_GENERATIONS - 1:\n",
    "        print(f\"{generation:3d} | {gen_best_fitness:+.4f} | {gen_mean_fitness:+.4f} | {gen_std_fitness:.4f}\")\n",
    "    \n",
    "    # Selection and mutation\n",
    "    if generation < N_GENERATIONS - 1:  # Don't mutate after last generation\n",
    "        parents = select_parents(population, fitnesses, n_parents)\n",
    "        population = mutate_population(parents, POPULATION_SIZE, MUTATION_STD, mut_key)\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  Best fitness: {best_fitness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_header",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "Visualize learning progress and best network function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training curve\n",
    "ax = axes[0]\n",
    "ax.plot(best_fitness_history, label='Best fitness', linewidth=2, color='darkblue')\n",
    "ax.plot(mean_fitness_history, label='Mean fitness', linewidth=1.5, \n",
    "        color='steelblue', alpha=0.7, linestyle='--')\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('Utility (fitness)')\n",
    "ax.set_title('Evolution Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Best network function vs tanh\n",
    "ax = axes[1]\n",
    "s_bar_range = jnp.linspace(0, 1, 200)\n",
    "\n",
    "# Trained network\n",
    "best_params_dict = unflatten_params(best_params, param_template)\n",
    "f_trained = get_regulatory_function(model, best_params_dict)\n",
    "f_values = f_trained(s_bar_range)\n",
    "\n",
    "# Target: tanh-like lateral inhibition\n",
    "tanh_strength = 5.0\n",
    "f_target = jnp.tanh(-tanh_strength * (s_bar_range - 0.5))\n",
    "\n",
    "ax.plot(s_bar_range, f_values, label='Trained NN', linewidth=2.5, color='darkred')\n",
    "ax.plot(s_bar_range, f_target, label=f'Target (tanh, α={tanh_strength})', \n",
    "        linewidth=2, color='green', linestyle='--', alpha=0.7)\n",
    "ax.axhline(0, color='k', linewidth=0.5, alpha=0.3)\n",
    "ax.axvline(0.5, color='k', linewidth=0.5, alpha=0.3)\n",
    "ax.set_xlabel('Neighbor average $\\\\bar{s}$')\n",
    "ax.set_ylabel('$ds/dt = f(\\\\bar{s})$')\n",
    "ax.set_title('Learned Regulatory Function')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Results plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patterns_header",
   "metadata": {},
   "source": [
    "## Generated Patterns\n",
    "\n",
    "Test trained network: simulate multiple replicates and examine resulting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate patterns with trained network\n",
    "key, test_key = random.split(key)\n",
    "test_replicates = 30\n",
    "\n",
    "final_states = run_multiple_replicates(\n",
    "    f=f_trained,\n",
    "    n_cells=N_CELLS,\n",
    "    n_replicates=test_replicates,\n",
    "    n_steps=N_STEPS,\n",
    "    dt=DT,\n",
    "    noise_strength=NOISE_STRENGTH,\n",
    "    key=test_key\n",
    ")\n",
    "\n",
    "patterns = apply_threshold(final_states)\n",
    "\n",
    "# Compute final utility\n",
    "u_soft, s_pat_soft, s_rep_soft = compute_soft_utility(patterns, bandwidth=SOFT_BANDWIDTH)\n",
    "u_hard, s_pat_hard, s_rep_hard = compute_hard_utility(patterns)\n",
    "\n",
    "print(\"Trained Network Performance:\")\n",
    "print(f\"  Soft utility: U = {u_soft:.4f} (S_pat={s_pat_soft:.4f}, S_rep={s_rep_soft:.4f})\")\n",
    "print(f\"  Hard utility: U = {u_hard:.4f} (S_pat={s_pat_hard:.4f}, S_rep={s_rep_hard:.4f})\")\n",
    "print(f\"\\nPattern statistics:\")\n",
    "print(f\"  Mean fate 1 ratio: {jnp.mean(patterns):.3f}\")\n",
    "print(f\"  Pattern diversity: {len(jnp.unique(patterns, axis=0))} unique / {test_replicates} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patterns\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot as heatmap\n",
    "im = ax.imshow(patterns, cmap='RdBu_r', aspect='auto', interpolation='nearest')\n",
    "ax.set_xlabel('Cell index')\n",
    "ax.set_ylabel('Replicate')\n",
    "ax.set_title(f'Trained Network Patterns (U={u_hard:.3f})')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Cell fate')\n",
    "cbar.set_ticks([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/trained_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print first few patterns\n",
    "print(\"\\nFirst 10 patterns:\")\n",
    "for i in range(min(10, test_replicates)):\n",
    "    pattern_str = ''.join(str(int(x)) for x in patterns[i])\n",
    "    print(f\"  {i:2d}: {pattern_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "## Comparison: Random vs Trained\n",
    "\n",
    "Compare trained network against random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random network for comparison\n",
    "key, random_key = random.split(key)\n",
    "random_params = init_params(model, random_key, (1,))\n",
    "f_random = get_regulatory_function(model, random_params)\n",
    "\n",
    "# Generate patterns with random network\n",
    "key, test_key = random.split(key)\n",
    "random_states = run_multiple_replicates(\n",
    "    f=f_random,\n",
    "    n_cells=N_CELLS,\n",
    "    n_replicates=test_replicates,\n",
    "    n_steps=N_STEPS,\n",
    "    dt=DT,\n",
    "    noise_strength=NOISE_STRENGTH,\n",
    "    key=test_key\n",
    ")\n",
    "random_patterns = apply_threshold(random_states)\n",
    "u_random, s_pat_random, s_rep_random = compute_hard_utility(random_patterns)\n",
    "\n",
    "# Comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Random function\n",
    "ax = axes[0]\n",
    "f_random_vals = f_random(s_bar_range)\n",
    "ax.plot(s_bar_range, f_random_vals, label='Random NN', linewidth=2, color='gray')\n",
    "ax.plot(s_bar_range, f_target, label='Target (tanh)', linewidth=1.5, \n",
    "        color='green', linestyle='--', alpha=0.7)\n",
    "ax.axhline(0, color='k', linewidth=0.5, alpha=0.3)\n",
    "ax.set_xlabel('$\\\\bar{s}$')\n",
    "ax.set_ylabel('$f(\\\\bar{s})$')\n",
    "ax.set_title('Random Network')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Trained function\n",
    "ax = axes[1]\n",
    "ax.plot(s_bar_range, f_values, label='Trained NN', linewidth=2, color='darkred')\n",
    "ax.plot(s_bar_range, f_target, label='Target (tanh)', linewidth=1.5, \n",
    "        color='green', linestyle='--', alpha=0.7)\n",
    "ax.axhline(0, color='k', linewidth=0.5, alpha=0.3)\n",
    "ax.set_xlabel('$\\\\bar{s}$')\n",
    "ax.set_ylabel('$f(\\\\bar{s})$')\n",
    "ax.set_title('Trained Network')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Utility comparison\n",
    "ax = axes[2]\n",
    "categories = ['Random', 'Trained']\n",
    "utilities = [u_random, u_hard]\n",
    "colors = ['gray', 'darkred']\n",
    "\n",
    "bars = ax.bar(categories, utilities, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Utility')\n",
    "ax.set_title('Performance Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for bar, util in zip(bars, utilities):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{util:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/random_vs_trained.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPerformance gain: {(u_hard - u_random):.4f} ({100*(u_hard-u_random)/abs(u_random):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## Save Trained Parameters\n",
    "\n",
    "Save best network for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dict = {\n",
    "    'params': best_params_dict,\n",
    "    'fitness': best_fitness,\n",
    "    'hidden_dims': HIDDEN_DIMS,\n",
    "    'training_config': {\n",
    "        'n_cells': N_CELLS,\n",
    "        'n_replicates': N_REPLICATES,\n",
    "        'population_size': POPULATION_SIZE,\n",
    "        'n_generations': N_GENERATIONS,\n",
    "        'mutation_std': MUTATION_STD\n",
    "    },\n",
    "    'fitness_history': {\n",
    "        'best': best_fitness_history,\n",
    "        'mean': mean_fitness_history\n",
    "    }\n",
    "}\n",
    "\n",
    "save_path = '../figures/trained_network.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "print(f\"✓ Trained parameters saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training approach**: Simple evolution strategy (μ+λ)-ES with Gaussian mutations\n",
    "\n",
    "**Key findings**:\n",
    "- Trained network should converge toward tanh-like function\n",
    "- Higher utility indicates better pattern formation (more information, less noise)\n",
    "- Expect emergence of alternating on-off patterns (lateral inhibition)\n",
    "\n",
    "**Next steps**: Notebook 05 for detailed analysis and visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
